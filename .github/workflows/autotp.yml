name: autotp

on:
  workflow_dispatch:
  merge_group:
    branches: [ master ]


concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  tests:
    runs-on: [self-hosted]

    steps:
      - uses: actions/checkout@v3

      - id: setup-venv
        uses: ./.github/workflows/setup-venv

      - name: Install essential packages
        run: |
          sudo apt-get install -y git cmake numactl iputils-ping

      - name: Check python version
        run: |
          python --version

      - name: Install oneCCL Bindings for PyTorch
        run: |
          python -m pip install torch
          #python -m pip install intel_extension_for_pytorch

          wget https://intel-extension-for-pytorch.s3.amazonaws.com/ipex_stable/cpu/oneccl_bind_pt-2.2.0%2Bcpu-cp310-cp310-linux_x86_64.whl
          pip install oneccl_bind_pt-2.2.0+cpu-cp310-cp310-linux_x86_64.whl
          python -m pip install oneccl_bind_pt -f https://developer.intel.com/ipex-whl-stable-cpu
          pip install py-cpuinfo

      - name: Install oneCCL
        run: |
          git clone https://github.com/oneapi-src/oneCCL
          cd oneCCL
          mkdir build
          cd build
          cmake ..
          make -j install

      - name: Install transformers
        run: |
          git clone https://github.com/huggingface/transformers
          cd transformers
          git rev-parse --short HEAD
          pip install .

      - name: Install deepspeed
        run: |
          #python -c "import torch;import intel_extension_for_pytorch as ipex;print(ipex._C._has_xpu())"
          # check why the host does not have AVX2 support
          pip install .[dev,1bit,autotuning,inf]
          ds_report

      - name: Python environment
        run: |
          pip list

      - name: Download DeepSpeedExamples
        run: |
          #git clone https://github.com/delock/DeepSpeedExamples --branch gma/hf_compare
          git clone https://github.com/foin6/DeepSpeedExamples --branch dev

      - name: Sanity check minimal
        run: |
          source oneCCL/build/_install/env/setvars.sh
          #python -c "import torch;import intel_extension_for_pytorch as ipex;print(ipex._C._has_xpu())"

      - name: AutoTP test (facebook/opt-1.3b)
        run: |
          # modify MODEL to change the model name, other lines are the same
          export MODEL=facebook/opt-1.3b
          source oneCCL/build/_install/env/setvars.sh
          cd DeepSpeedExamples/inference/huggingface/text-generation
          DS_ACCELERATOR=cpu TRANSFORMERS_CACHE=/tmp/transformers_cache/ deepspeed --num_gpus 2 --bind_cores_to_rank inference-test.py --model $MODEL --dtype float32 --use_meta_tensor
#          TRANSFORMERS_CACHE=/tmp/transformers_cache/ deepspeed --num_gpus 2 --bind_cores_to_rank ds-hf-compare.py --model $MODEL --dtype float32 --num_inputs 200 --use_kernel False
#
#      - name: AutoTP test (bigscience/bloom-3b)
#        run: |
#          # modify MODEL to change the model name, other lines are the same
#          export MODEL=bigscience/bloom-3b
#          source oneCCL/build/_install/env/setvars.sh
#          cd DeepSpeedExamples/inference/huggingface/text-generation
#          TRANSFORMERS_CACHE=/tmp/transformers_cache/ deepspeed --num_gpus 2 --bind_cores_to_rank inference-test.py --model $MODEL --dtype float32 --use_meta_tensor
#          TRANSFORMERS_CACHE=/tmp/transformers_cache/ deepspeed --num_gpus 2 --bind_cores_to_rank ds-hf-compare.py --model $MODEL --dtype float32 --num_inputs 200 --use_kernel False
